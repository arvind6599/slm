{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Make a questionnaire that asks questions regarding \n",
    "\n",
    "-- Use-ases for shortcuts to assign macros to either mouse buttons or keybaord keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import streamlit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydantic\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from prompt_library import *\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" You are an AI Assistant asking questions to the user based on the following theme: What is your name?. Adapt the question based on the user's past response. Ask questions using the following format:\\n\\n    Question: ...\\n\\n    Options:\\n    ...\\n    ...\\n    ...\\n    ...\\n    \""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_LIBRARY[\"QUESTIONING\"].format(system_prompt= \"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ollama generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionSchema(pydantic.BaseModel):\n",
    "    question: str\n",
    "    option_1: str\n",
    "    option_2: str\n",
    "    option_3: str\n",
    "    option_4: str\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.question} \\n A: {self.option_1} \\n B: {self.option_2} \\n C: {self.option_3} \\n D: {self.option_4}\"\n",
    "\n",
    "def ask_question(model, messages):\n",
    "\n",
    "    response_text = ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        format=QuestionSchema.model_json_schema(),\n",
    "    )['message']['content']\n",
    "\n",
    "    return QuestionSchema.model_validate_json(response_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you tell me more about your favorite hobby or activity that brings you joy? \n",
      " A: I really enjoy hiking in nature \n",
      " B: I love cooking gourmet meals for friends and family \n",
      " C: I prefer playing video games alone \n",
      " D: I like collecting stamps\n",
      "Answering: I love cooking gourmet meals for friends and family\n",
      "What type of cuisines do you prefer when cooking gourmet meals? \n",
      " A: Italian \n",
      " B: Japanese \n",
      " C: Indian \n",
      " D: Mexican\n",
      "Answering: Italian\n",
      "Which Italian dishes are some of your favorites to cook? \n",
      " A: Pizza \n",
      " B: Lasagna \n",
      " C: Risotto \n",
      " D: Paella\n",
      "Answering: Pizza\n",
      "What types of pizza toppings do you like to use in your homemade pizzas? \n",
      " A: Ham and pineapple \n",
      " B: Peppers and onions \n",
      " C: BBQ sauce and beef \n",
      " D: Mushrooms and olives\n",
      "Answering: Peppers and onions\n",
      "Are there any specific ingredients or recipes that you find particularly challenging when cooking gourmet meals? \n",
      " A: Yes, I struggle with making authentic Italian pasta sauces. \n",
      " B: I have difficulty finding the right spices for Mexican dishes. \n",
      " C: It's hard for me to make a perfect pizza crust from scratch. \n",
      " D: None of these apply to my cooking challenges.\n",
      "Answering: Yes, I struggle with making authentic Italian pasta sauces.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "MODEL = \"qwen2.5:3b\"\n",
    "messages = [\n",
    "    {\"role\":\"system\", \"content\":PROMPT_LIBRARY[\"QUESTIONING\"]}\n",
    "]\n",
    "\n",
    "questions = []\n",
    "\n",
    "while(len(questions) < 5):\n",
    "    \n",
    "    output = ask_question(MODEL, messages)\n",
    "    questions.append(output.question)\n",
    "    print(output)\n",
    "    messages.append({\"role\":\"assistant\", \"content\":output.question})\n",
    "    options = [output.option_1, output.option_2, output.option_3, output.option_4]\n",
    "    response = random.choice(options)\n",
    "    print(\"Answering:\", response)\n",
    "    messages.append({\"role\":\"user\", \"content\":response})\n",
    "\n",
    "    # time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \" You are an AI Assistant asks random fact based questions to test the knowledge of the user. Based on the user's response, the AI will ask another question. The conversation will continue until the AI has asked 10 questions. At the end of the conversation, the AI will provide a summary of the questions and answers. The user can then choose to start a new conversation or end the session. The AI will ask questions using the following format:\\n\\n    Question: ...\\n\\n    Options:\\n    ...\\n    ...\\n    ...\\n    ...\\n    \"}, {'role': 'assistant', 'content': 'What is the largest desert in the world by area?'}, {'role': 'user', 'content': 'Kalahari'}, {'role': 'assistant', 'content': 'What is the largest desert in the world by area?'}, {'role': 'user', 'content': 'Kalahari'}, {'role': 'assistant', 'content': 'What is the largest desert in the world by area?'}, {'role': 'user', 'content': 'Sahara'}, {'role': 'assistant', 'content': 'What is the largest desert in the world by area?'}, {'role': 'user', 'content': 'Antarctica'}, {'role': 'assistant', 'content': 'What is the largest desert in the world by area?'}, {'role': 'user', 'content': 'Sahara'}]\n"
     ]
    }
   ],
   "source": [
    "print(str(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the functions to do RAG on for further questioning and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6e0271289744aa97a118473acbb7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arvin\\anaconda3\\envs\\mnlp-a2\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arvin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bca955c670c400faefecb3888bd64a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0752c0f9bca542578e6215c02b895140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13db9e24ef4c4e03890852c1e3ae3075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843ec0a4c72c47f6aefc896ab7a18f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341749cc4de64d3a9c454b822468e07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986ab4ab292e475889d4f5704ef5aa3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea7ff357a7a4b8fad8cbdaa34123003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ef5d940e1d48f1b538e6bd5639fad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3850d46f994fe0a6bff27ce0343778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ba6e76497d438f9f24b071ac31ec80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arvin\\anaconda3\\envs\\mnlp-a2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "functions = [\n",
    "    {\"name\": \"add_numbers\", \"description\": \"Adds two numbers and returns the result.\"},\n",
    "    {\"name\": \"subtract_numbers\", \"description\": \"Subtracts the second number from the first.\"},\n",
    "    {\"name\": \"multiply_numbers\", \"description\": \"Multiplies two numbers and returns the result.\"},\n",
    "    {\"name\": \"divide_numbers\", \"description\": \"Divides the first number by the second, returns the quotient.\"}\n",
    "]\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode function descriptions\n",
    "function_embeddings = {f[\"name\"]: model.encode(f[\"description\"]) for f in functions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant function is 'subtract_numbers'. Confidence: 0.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_function(query):\n",
    "    query_embedding = model.encode(query)\n",
    "    similarities = {name: cosine_similarity([query_embedding], [embedding])[0][0]\n",
    "                    for name, embedding in function_embeddings.items()}\n",
    "    best_match = max(similarities, key=similarities.get)\n",
    "    return best_match, similarities[best_match]\n",
    "\n",
    "def generate_response(query):\n",
    "    function_name, confidence = retrieve_function(query)\n",
    "    return f\"The most relevant function is '{function_name}'. Confidence: {confidence:.2f}\"\n",
    "\n",
    "\n",
    "\n",
    "query = \"What is 4/2?\"\n",
    "response = generate_response(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logitech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
